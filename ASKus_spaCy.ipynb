{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASKus.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FZUIqJj0TLx"
      },
      "source": [
        "# SpaCy Named Entity Recognition Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_une_bE0aS3"
      },
      "source": [
        "## (Professor/TA skip) Mount drive with data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxRaz1CSRANm"
      },
      "source": [
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEBFf0C0RQS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9e119b-ee57-4f45-d08e-c1a16fa70c33"
      },
      "source": [
        "#AT/TA skip\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfCXaaGFRXQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7bc6530-2596-4362-b417-7ca21134fce3"
      },
      "source": [
        "#AT/TA skip\n",
        "%cd /content/drive/Shared\\ drives/256Group10/ASKus\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/256Group10/ASKus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGboZMKz0pOB"
      },
      "source": [
        "## Data Exploration and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXe22Rkp0yuw"
      },
      "source": [
        "### Load train.csv containing matrix with paper id's corresponding to dataset label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKYBGIhzgC24"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import fileinput\n",
        "import time\n",
        "\n",
        "import ast\n",
        "\n",
        "import re\n",
        "\n",
        "import spacy as sp\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "import random\n",
        "\n",
        "import json"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYY3DmMQpvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "481708e2-28f6-4b78-fbbd-82210a4c2f28"
      },
      "source": [
        "\n",
        "\n",
        "train = pd.read_csv('./data/train.csv')\n",
        "train"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n",
              "      <td>The Impact of Dual Enrollment on College Degre...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n",
              "      <td>Educational Attainment of High School Dropouts...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n",
              "      <td>Differences in Outcomes for Female and Male St...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n",
              "      <td>Stepping Stone and Option Value in a Model of ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
              "      <td>Parental Effort, School Resources, and Student...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19656</th>\n",
              "      <td>b3498176-8832-4033-aea6-b5ea85ea04c4</td>\n",
              "      <td>RSNA International Trends: A Global Perspectiv...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "      <td>RSNA International COVID Open Radiology Database</td>\n",
              "      <td>rsna international covid open radiology database</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19657</th>\n",
              "      <td>f77eb51f-c3ac-420b-9586-cb187849c321</td>\n",
              "      <td>MCCS: a novel recognition pattern-based method...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19658</th>\n",
              "      <td>ab59bcdd-7b7c-4107-93f5-0ccaf749236c</td>\n",
              "      <td>Quantitative Structure–Activity Relationship M...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19659</th>\n",
              "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
              "      <td>A ligand-based computational drug repurposing ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19660</th>\n",
              "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
              "      <td>A ligand-based computational drug repurposing ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds data</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19661 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Id  ...                                      cleaned_label\n",
              "0      d0fa7568-7d8e-4db9-870f-f9c6f668c17b  ...              national education longitudinal study\n",
              "1      2f26f645-3dec-485d-b68d-f013c9e05e60  ...              national education longitudinal study\n",
              "2      c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29  ...              national education longitudinal study\n",
              "3      5c9a3bc9-41ba-4574-ad71-e25c1442c8af  ...              national education longitudinal study\n",
              "4      c754dec7-c5a3-4337-9892-c02158475064  ...              national education longitudinal study\n",
              "...                                     ...  ...                                                ...\n",
              "19656  b3498176-8832-4033-aea6-b5ea85ea04c4  ...   rsna international covid open radiology database\n",
              "19657  f77eb51f-c3ac-420b-9586-cb187849c321  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19658  ab59bcdd-7b7c-4107-93f5-0ccaf749236c  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19659  fd23e7e0-a5d2-4f98-992d-9209c85153bb  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19660  fd23e7e0-a5d2-4f98-992d-9209c85153bb  ...    cas covid 19 antiviral candidate compounds data\n",
              "\n",
              "[19661 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vhBtKXJ1HQ"
      },
      "source": [
        "### **Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "3PJ54ohUJ0Je",
        "outputId": "908905b7-7d65-4aed-847b-9c257a693724"
      },
      "source": [
        "# start with a single file\n",
        "trainJSON = pd.read_json('./data/train/ef0a0fb9-5b60-4e79-90d0-6c8d71e6aa48.json')\n",
        "trainJSON"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>section_title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract</td>\n",
              "      <td>A number of recent studies have not replicated...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>To replicate an experiment in human biology an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  section_title                                               text\n",
              "0      Abstract  A number of recent studies have not replicated...\n",
              "1                To replicate an experiment in human biology an..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVsLVqizK8ly"
      },
      "source": [
        "# now get the pathnames of all of the JSON files into an array\n",
        "path_to_json = './data/train/'\n",
        "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv-AMF3CHpeC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "faff8f6f-ad47-460f-c239-3827d3a1905d"
      },
      "source": [
        "# create the arrays to be filled\n",
        "## will be filled with all of the pathnames names of each file\n",
        "filenames = []\n",
        "## will contain all of the text per file\n",
        "texts = []\n",
        "## will contain the title or id of each of the files\n",
        "titles = []\n",
        "## will contain the number of sections per document\n",
        "sections = []\n",
        "\n",
        "# for every file in the array...\n",
        "for file in json_files:\n",
        "  # create the path to the file\n",
        "  filepath = path_to_json + file\n",
        "  # read the file into a pandas dataframe\n",
        "  file_df = pd.read_json(filepath)\n",
        "  # extract and concate all of the text into a single varaible\n",
        "  text = file_df[\"text\"].str.cat()\n",
        "  # extract the title or id from the path\n",
        "  title = file.split(\".json\")[0]\n",
        "  # get the number of sections in the dataframe\n",
        "  section = len(file_df)\n",
        "  \n",
        "  # append to data to its respective array\n",
        "  sections.append(section)\n",
        "  titles.append(title)\n",
        "  filenames.append(file)\n",
        "  texts.append(text)\n",
        "  \n",
        "\n",
        "# create a new data frame and fill it with data from the arrays\n",
        "dtm_df = pd.DataFrame()\n",
        "dtm_df.insert(0, \"id\", titles)\n",
        "dtm_df.insert(1, \"sections\", sections)\n",
        "dtm_df.insert(2, \"text\", texts)\n",
        "  \n",
        "# show the dataframe\n",
        "dtm_df\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sections</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ef0a0fb9-5b60-4e79-90d0-6c8d71e6aa48</td>\n",
              "      <td>2</td>\n",
              "      <td>A number of recent studies have not replicated...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ef1770e2-527a-4549-b345-ee002246af1f</td>\n",
              "      <td>36</td>\n",
              "      <td>Industrial policies that subsidize (often larg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ef19c53d-899a-41ba-bb2a-6b03c9c97497</td>\n",
              "      <td>11</td>\n",
              "      <td>Obsessive-compulsive disorder (OCD) is a heter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ef1a2ad5-73ea-42a6-839b-1092efa5e8ad</td>\n",
              "      <td>9</td>\n",
              "      <td>. Map Sprague's Pipits breed from north-centra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ef228135-27af-4278-a225-57acd2ac0a82</td>\n",
              "      <td>7</td>\n",
              "      <td>most common observed substitutions in SARS-CoV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>04e8222c-ff33-4881-9b80-7121063cde0c</td>\n",
              "      <td>9</td>\n",
              "      <td>Recent public health research documents geogra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>04efcc82-9f89-4b40-a506-3a7cb34b5242</td>\n",
              "      <td>22</td>\n",
              "      <td>R epresentation by race and ethnicity is one o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>04f0863b-54fa-4ae5-ad0e-a9b4d42e2a29</td>\n",
              "      <td>9</td>\n",
              "      <td>[1] Numerical model experiments are conducted ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>04fd49ad-466e-4d07-8848-2158e4994515</td>\n",
              "      <td>10</td>\n",
              "      <td>Strong decadal variations in the oceanic uptak...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>05004e80-e1d2-414c-9473-5e7b4b3840ff</td>\n",
              "      <td>6</td>\n",
              "      <td>Cancer is a growth process and it is natural t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  ...                                               text\n",
              "0      ef0a0fb9-5b60-4e79-90d0-6c8d71e6aa48  ...  A number of recent studies have not replicated...\n",
              "1      ef1770e2-527a-4549-b345-ee002246af1f  ...  Industrial policies that subsidize (often larg...\n",
              "2      ef19c53d-899a-41ba-bb2a-6b03c9c97497  ...  Obsessive-compulsive disorder (OCD) is a heter...\n",
              "3      ef1a2ad5-73ea-42a6-839b-1092efa5e8ad  ...  . Map Sprague's Pipits breed from north-centra...\n",
              "4      ef228135-27af-4278-a225-57acd2ac0a82  ...  most common observed substitutions in SARS-CoV...\n",
              "...                                     ...  ...                                                ...\n",
              "14311  04e8222c-ff33-4881-9b80-7121063cde0c  ...  Recent public health research documents geogra...\n",
              "14312  04efcc82-9f89-4b40-a506-3a7cb34b5242  ...  R epresentation by race and ethnicity is one o...\n",
              "14313  04f0863b-54fa-4ae5-ad0e-a9b4d42e2a29  ...  [1] Numerical model experiments are conducted ...\n",
              "14314  04fd49ad-466e-4d07-8848-2158e4994515  ...  Strong decadal variations in the oceanic uptak...\n",
              "14315  05004e80-e1d2-414c-9473-5e7b4b3840ff  ...  Cancer is a growth process and it is natural t...\n",
              "\n",
              "[14316 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djLO7laQUvVv"
      },
      "source": [
        "# create the arrays to be filled\n",
        "sentances = []\n",
        "words = []\n",
        "\n",
        "# for every row in the data frame\n",
        "for field, row in dtm_df.items():\n",
        "  # only work with the text column for each row\n",
        "  if(field == 'text'):\n",
        "    # for each row within the text column\n",
        "    for textbook in range(0, 14316):\n",
        "      # create an array of sentances\n",
        "      arrayOfSentances = row.get(textbook).split(\".\") \n",
        "      # add the number of sentances into respective array\n",
        "      sentances.append(len(arrayOfSentances))\n",
        "      # create word count variable\n",
        "      word = 0\n",
        "      # for each sentance in the text\n",
        "      for sentance in arrayOfSentances:\n",
        "        # get the number of words in the sentance\n",
        "        result = len(sentance.split())\n",
        "        # add to the running total of wordcount\n",
        "        word = word + result\n",
        "      # add the number of words into respective array\n",
        "      words.append(word)\n",
        "\n",
        "# Print data \n",
        "# print(\"The number of sentances per article are: \")\n",
        "# print(sentances)\n",
        "# print(\"The number of words per article are: \")\n",
        "# print(words)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4TS7F6j0qO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c48ad27-2e35-405b-8909-6f58460b20ee"
      },
      "source": [
        "# get the average words, sentances, and sections per artcle\n",
        "def AverageData(arr):\n",
        "  return sum(arr)/len(arr)\n",
        "averageWords = AverageData(words)\n",
        "averageSenences = AverageData(sentances)\n",
        "averageSections = AverageData(sections)\n",
        "\n",
        "print(\"The Average Words per article are: \", averageWords)\n",
        "print(\"The Average Sentences per article are: \", averageSenences)\n",
        "print(\"The Average Sections per article are: \", averageSections)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Average Words per article are:  7144.80546241967\n",
            "The Average Sentences per article are:  577.8894244202291\n",
            "The Average Sections per article are:  18.071668063704944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y1Ofn3aSDhc"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B-Hz-XvSD8z"
      },
      "source": [
        "## SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAR1lo__RQiU"
      },
      "source": [
        "##script used to take json text files and train.csv and convert them to SpaCy friendly format (takes approx. 80 minutes to complete)\n",
        "\n",
        "##create array to store tuples of publication text and entity (dataset name) locations (this is how SpaCy wants it)\n",
        "# SPACY_TRAIN = []\n",
        "\n",
        "##counter used to track progress\n",
        "# count = 0\n",
        "\n",
        "##loop through train.csv rows\n",
        "# for index, row in train.iterrows():\n",
        "#   print(count)\n",
        "#   this_pub = \"\"\n",
        "\n",
        "##load json file corresponding to publication id in the train.csv row, store known dataset label\n",
        "#   this_pub_id = './data/train/' + row['Id'] + '.json'\n",
        "#   this_data_lab = row['dataset_label']\n",
        "#   this_pub_json = json.load(open(this_pub_id))\n",
        "\n",
        "##append all text from the publication in to one (super-long!) string\n",
        "#   for section in this_pub_json:\n",
        "#     this_pub += section['text']\n",
        "\n",
        "##Find locations of dataset in publications and store in an entity dictionary\n",
        "#   this_ent_dict = {\"entities\": []}\n",
        "#   for m in re.finditer(this_data_lab, this_pub):\n",
        "#     this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "\n",
        "## append the tuple of the entire publication text and the entity locations in the text\n",
        "#   SPACY_TRAIN.append((this_pub, this_ent_dict))\n",
        "#   count+=1\n",
        "\n",
        "# SPACY_TRAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u4Xm-duW8pX"
      },
      "source": [
        "#write SpaCy friendly training data into a text file\n",
        "\n",
        "# file = open(\"./data/spacy/spacy_train.txt\", \"w\")\n",
        "# SPACY_TRAIN_File = repr(SPACY_TRAIN)\n",
        "# file.write(SPACY_TRAIN_File)\n",
        "# file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpq9Wa3Gt1Cw"
      },
      "source": [
        "#script to read in SpaCy friendly data from text file (script modified from one here: https://www.geeksforgeeks.org/how-to-read-large-text-files-in-python/)\n",
        "  \n",
        "#time at the start of program is noted\n",
        "start = time.time()\n",
        "spacy_train = \"\"\n",
        "#keeps a track of number of lines in the file\n",
        "count = 0\n",
        "for lines in fileinput.input([\"./data/spacy/spacy_train.txt\"]):\n",
        "    spacy_train = lines\n",
        "    count = count + 1\n",
        "      \n",
        "#time at the end of program execution is noted\n",
        "end = time.time()\n",
        "  \n",
        "#total time taken to print the file\n",
        "print(\"Execution time in seconds: \",(end - start))\n",
        "print(\"No. of lines printed: \",count)\n",
        "\n",
        "#number of chars in file (exceeds one billion)\n",
        "print(\"Number of characters in the data file (too many): \", len(spacy_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYMJcTI7NdDO"
      },
      "source": [
        "#SpaCy friendly training data was read in from .txt file as a string - use following to convert back to list of tuples\n",
        "#Source code: https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list\n",
        "\n",
        "DATA = ast.literal_eval(spacy_train)\n",
        "\n",
        "print(\"number of (paper, dataset_location) tuples in the data: \", len(DATA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npbPApGyIJVe"
      },
      "source": [
        "#split data into 70% training and 30% test\n",
        "#Train only on the 70% and keep the rest for evaluation purposes\n",
        "#As model training and internal evaluation was done over multiple sessions, this was run once and the separate\n",
        "# train and test data were saved to our shared Google Drive to maintain the integrity of our evaluation results,\n",
        "# hence it is commented out here \n",
        "\n",
        "# #randomly shuffle the tuples\n",
        "random.shuffle(DATA)\n",
        "\n",
        "train_mark = (len(DATA)*7//10)\n",
        "TRAIN_FULL = DATA[:train_mark]\n",
        "TEST_FULL = DATA[train_mark:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQJM72AqAWrr"
      },
      "source": [
        "# write train/test split into a text file, as model will be trained on this split\n",
        "\n",
        "#As model training and internal evaluation was done over multiple sessions, this was run once and the separate\n",
        "# train and test data were saved to our shared Google Drive to maintain the integrity of our evaluation results,\n",
        "# hence it is commented out here \n",
        "\n",
        "# file = open(\"./data/spacy/TRAIN_FULL.txt\", \"w\")\n",
        "# TRAIN_FULL_SET = repr(TRAIN_FULL)\n",
        "# file.write(TRAIN_FULL_SET)\n",
        "# file.close()\n",
        "\n",
        "# file = open(\"./data/spacy/TEST_FULL.txt\", \"w\")\n",
        "# TEST_FULL_SET = repr(TEST_FULL)\n",
        "# file.write(TEST_FULL_SET)\n",
        "# file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svLcu6si-FCA"
      },
      "source": [
        "#Upon attempting training, there was not enough RAM/it was too slow to process all texts in their entirities.\n",
        "#As such, following script is used to compress the training data.  It breaks the publications up into sentences.  It then picks,\n",
        "# from each publication, one sentence that contains the data set name inside it, and one sentence that does not.\n",
        "\n",
        "#set our custom entity label to be trained on later\n",
        "LABEL = 'data'\n",
        "\n",
        "TRAIN_DATA = []\n",
        "\n",
        "#loop through tuples in original data\n",
        "#hit_count and miss_count serve as counters for how many sentences we have already extracted from this particular paper\n",
        "for paper in TRAIN_FULL:\n",
        "  hit_count = 0\n",
        "  miss_count = 0\n",
        "\n",
        "  #determine if the paper we are looking at contains any entities\n",
        "  entities = paper[1]['entities']\n",
        "  if len(entities) > 0:\n",
        "    #store the dataset name for this publication\n",
        "    dataset = paper[0][entities[0][0]:entities[0][1]-1]\n",
        "\n",
        "  #split data into sentences, approximately (based on the '.' puncutation mark)\n",
        "  sentences = paper[0].split('.')\n",
        "\n",
        "  #loop through sentences\n",
        "  for sentence in sentences:\n",
        "    this_ent_dict = {\"entities\": []}\n",
        "    if dataset in sentence:\n",
        "      hit_count +=1\n",
        "\n",
        "      #find all occurences of explicit mentions of the dataset in this sentence\n",
        "      for m in re.finditer(dataset,sentence):\n",
        "        this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "      #compressed data: append a tuple of just the sentence (as opposed to the entire paper) containing the dataset entity\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "    \n",
        "    #after find one sentence containing a dataset, append two that do not contain it\n",
        "    elif hit_count > 0 and miss_count <= hit_count:\n",
        "      miss_count += 1\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "\n",
        "    #when have two sentences from this paper (one containing dataset and one without), move on to next paper\n",
        "    if hit_count >= 1 and miss_count >= 2:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drMyeKVVQeJA"
      },
      "source": [
        "#number of entries ((sentence, entity-locations) tuples) in the compressed dataset\n",
        "print(\"number of (sentence, datset_location) tuples in the compressed data: \", len(TRAIN_DATA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bS0eO6WFv1h"
      },
      "source": [
        "#find and remove all sentences containing no chars (ex: \"\", \" \", \"      \") as these cause an error in the SpaCy training algorithm\n",
        "\n",
        "count = 0\n",
        "sen = 0\n",
        "while sen < len(TRAIN_DATA):\n",
        "  #if the sentence contains some chars, move on to the next sentence\n",
        "  if TRAIN_DATA[sen][0].strip():\n",
        "      sen += 1\n",
        "  \n",
        "  #if it is a blank sentence, delete it\n",
        "  else:\n",
        "      # print(TRAIN_DATA[sen])\n",
        "      del TRAIN_DATA[sen]\n",
        "      count+=1\n",
        "\n",
        "\n",
        "print(\"Number of blank sentences we removed: \", count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgw5tlXP7rN7"
      },
      "source": [
        "## Training SpaCy NER with Custom Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jo4O0XQYzbG"
      },
      "source": [
        "#script derived from https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
        "\n",
        "\n",
        "#create empty model\n",
        "nlp = sp.blank('en')\n",
        "print(\"created blank english model\")\n",
        "\n",
        "#add ner (entity recognizer) to the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "#add our custom entity label to the entity recognizer\n",
        "ner.add_label(LABEL)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "#set number of epochs we want our model to perform\n",
        "n_iter = 10\n",
        "lossList = []\n",
        "#make sure we only train our ner pipe to speed up processing\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    for itn in range(n_iter):\n",
        "        print(itn)\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        #split data into batches of 500 sentences\n",
        "        batches = minibatch(TRAIN_DATA, size=500)\n",
        "        for batch in batches:\n",
        "            print('batch size', len(batch))\n",
        "            #extract the sentence and the entities it contains for each sentence\n",
        "            texts, annotations = zip(*batch)\n",
        "            #update the weights\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
        "                        losses=losses)\n",
        "        #loss after this epoch\n",
        "        lossList.append(losses['ner'])\n",
        "        print('Losses', losses)\n",
        "\n",
        "\n",
        "print(lossList)\n",
        "# Save model \n",
        "output_dir = Path('./models/spacy')\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir()\n",
        "nlp.meta['name'] = 'SpaCy'  # rename model\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQYAyPPaEP9j"
      },
      "source": [
        "## Internal Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7RMBmK6OZgy"
      },
      "source": [
        "# code reference: https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "plt.plot(x, lossList)\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('Iteration')\n",
        "# naming the y axis\n",
        "plt.ylabel('NER Losses')\n",
        "  \n",
        "# giving a title to my graph\n",
        "plt.title('My first graph!')\n",
        "  \n",
        "# function to show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89iUoceWfwio"
      },
      "source": [
        "#Load \"test\" set (30% training data held aside above)\n",
        "#Note: these are not compressed into sentences, it is full texts.  The evaluation script below breaks into sentences\n",
        "\n",
        "#script to read in SpaCy friendly data from text file (script modified from one here: https://www.geeksforgeeks.org/how-to-read-large-text-files-in-python/)\n",
        "  \n",
        "#time at the start of program is noted\n",
        "start = time.time()\n",
        "TEST_FULL = \"\"\n",
        "#keeps a track of number of lines in the file\n",
        "count = 0\n",
        "for lines in fileinput.input([\"./data/spacy/TEST_FULL.txt\"]):\n",
        "    TEST_FULL = lines\n",
        "    count = count + 1\n",
        "      \n",
        "#time at the end of program execution is noted\n",
        "end = time.time()\n",
        "  \n",
        "#total time taken to print the file\n",
        "print(\"Execution time in seconds: \",(end - start))\n",
        "print(\"No. of lines printed: \",count)\n",
        "\n",
        "#number of chars in file (exceeds one billion)\n",
        "print(\"Number of characters in the data file : \", len(TEST_FULL))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMalJFigg4CY"
      },
      "source": [
        "#Test data was read in from .txt file as a string - use following to convert back to list of tuples\n",
        "#Source code: https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list\n",
        "\n",
        "TEST_FULL = ast.literal_eval(TEST_FULL)\n",
        "\n",
        "print(\"number of (paper, dataset_location) tuples in the data: \", len(TEST_FULL))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcSC--8rRGmJ"
      },
      "source": [
        "#Load SpaCy ner model trained on 70% of the data, evaluate on 5 1% slices of unseen test data (5% (300 full papers) used out of 30% (~6000 full papers) due to runtime constraints)\n",
        "\n",
        "# output_dir = Path('./models/spacy')\n",
        "# print(\"Loading from\", output_dir)\n",
        "# nlp2 = sp.load(output_dir)\n",
        "\n",
        "TP5 = 0\n",
        "FP5 = 0\n",
        "FN5 = 0\n",
        "precision5 = 0.0\n",
        "recall5 = 0.0\n",
        "micro = 0.0\n",
        "\n",
        "#Kaggle-provided Jaccard similarity function between strings\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "#function provided on Kaggle competition to clean dataset label for submission\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
        "\n",
        "#lists to hold actual dataset labels and predicted dataset labels across papers\n",
        "ground_truth = []\n",
        "predictions = []\n",
        "\n",
        "#counter used to track progress of the script\n",
        "count = 0\n",
        "\n",
        "#We are taking 1% of the test data to evaluate on.  This is approx. 60 papers.  \n",
        "# We chose this for a couple reasons.  1) Kaggle only asks us to submit predictions for 4 papers, 2) predicting on 60 papers vs. 6000 papers reduces runtime\n",
        "# from ~730 minutes to ~30 minutes.\n",
        "\n",
        "for itr in range(5):\n",
        "    #initialize variables to hold true positives (TP), false negatives (FN), and false positives (FP) (defined below)\n",
        "    TP=0\n",
        "    FP=0\n",
        "    FN=0\n",
        "\n",
        "    #randomly shuffle the tuples\n",
        "    random.shuffle(TEST_FULL)\n",
        "\n",
        "    #pick 60 papers at random from test set\n",
        "    start_range = random.randint(0, 5839)\n",
        "    end_range = start_range + 60\n",
        "    print('start_range', start_range)\n",
        "    print('end_range', end_range)\n",
        "\n",
        "    #The following loop creates two arrays, one with the actual dataset labels for each paper and one for the predicted labels.\n",
        "    # The order of each list matches with each other, which makes comparison easy later on.\n",
        "\n",
        "    #Loop through each (paper, dataset labels) tuple\n",
        "    for test_text in TEST_FULL[start_range:end_range]:\n",
        "        count +=1\n",
        "        print(count)\n",
        "\n",
        "        #following list stores actual entities for the current paper\n",
        "        this_gt = []\n",
        "\n",
        "        #loop through entities for this paper, store once\n",
        "        for gt_ent in test_text[1]['entities']:\n",
        "            dataset = test_text[0][gt_ent[0]:gt_ent[1]-1]\n",
        "            dataset = clean_text(dataset)\n",
        "            if dataset not in this_gt:\n",
        "                this_gt.append(dataset)\n",
        "        ground_truth.append(this_gt)\n",
        "\n",
        "        #this_pred stores all dataset label predictions for this paper\n",
        "        this_pred = []\n",
        "\n",
        "        #split current paper into sentences, approximately\n",
        "        sentences = test_text[0].split('.')\n",
        "\n",
        "        #for each sentence, predict dataset labels\n",
        "        for sentence in sentences:\n",
        "            doc = nlp(sentence)\n",
        "            #loop through entities found in this sentence, if they contain chars store them, make sure to only store once\n",
        "            for ent in doc.ents:\n",
        "                if len(ent.text) > 0:\n",
        "                    this_ent = clean_text(ent.text)\n",
        "                    #remove any trailing or leading white spaces\n",
        "                    this_ent = this_ent.strip()\n",
        "                    if this_ent not in this_pred:\n",
        "                        this_pred.append(this_ent)\n",
        "\n",
        "        predictions.append(this_pred)\n",
        "\n",
        "    #loop through the range of both arrays (ground_truth and prediction arrays are the same length)\n",
        "    for num in range(0, len(ground_truth)):\n",
        "        this_gt_set = ground_truth[num]\n",
        "\n",
        "        #Kaggle competition eval. page says: \"Predicted strings for each publication are sorted alphabetically and processed in that order. \n",
        "        #Any scoring ties are resolved on the basis of that sort.\" Hence we sort the prediction strings alphabetically\n",
        "        this_pred_set = sorted(predictions[num])\n",
        "\n",
        "        #keep track of which predictions were matched, unmatched predictions count as FP\n",
        "        matched_pred = []\n",
        "\n",
        "        #loop through ground truth predictions for this paper\n",
        "        for key in this_gt_set:\n",
        "            #max_sim keeps track of most simliar prediction to ground truth string\n",
        "            max_sim = 0\n",
        "            #loop through this papers predictions\n",
        "            for pred in this_pred_set:\n",
        "                this_sim = jaccard(key, pred)\n",
        "                #store prediction (mark as matched) if it has greater than 0 jaccard similarity to some ground truth dataset label\n",
        "                if this_sim > 0:\n",
        "                    if pred not in matched_pred:\n",
        "                        matched_pred.append(pred)\n",
        "                if this_sim > max_sim:\n",
        "                    max_sim = this_sim\n",
        "            #if a ground truth is predicted with greater than 0.5 similarity, mark as TP\n",
        "            if max_sim >= 0.5:\n",
        "                TP += 1\n",
        "            #if ground truth is only predicted with greater than 0 similarity but less than 0.5, count as FP\n",
        "            elif max_sim < 0.5 and max_sim > 0:\n",
        "                FP += 1\n",
        "            #if ground truth is not matched by any predictions, count as FN\n",
        "            elif max_sim == 0:\n",
        "                FN += 1\n",
        "            #all unmatched predicitons counted as FP\n",
        "            FP += len(this_pred_set) - len(matched_pred)\n",
        "    TP5 += TP\n",
        "    FP5 += FP\n",
        "    FN5 += FN\n",
        "    print('Total over 5 runs: ', TP5, FP5, FN5)\n",
        "    \n",
        "    ground_truth.clear()\n",
        "    predictions.clear()\n",
        "#Calculate precision, recall, and micro F0.5 scores\n",
        "TP = TP5/5\n",
        "FP = FP5/5\n",
        "FN = FN5/5\n",
        "print('Average TP5 FP5 FN5 ', TP5, FP5, FN5)\n",
        "\n",
        "precision = TP/(TP+FP)\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "recall = TP/(TP+FN)\n",
        "\n",
        "print(\"Recall: \", recall)\n",
        "\n",
        "microF = (2*precision*recall)/(precision+recall)\n",
        "print(\"Micro F0.5: \", microF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kptqWG9S3Pc"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# set width of bar\n",
        "plotData = {'TP' : 30, 'FP':5, 'FN':16}\n",
        "#make list for x and y axis\n",
        "evalMetrices = list(plotData.keys())\n",
        "evalValues = list(plotData.values())\n",
        " \n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "plt.bar(evalMetrices, evalValues, color ='maroon',\n",
        "        width = 0.4)\n",
        "\n",
        "plt.xlabel(\"Evaluation Metrices\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.title(\"Model Evaluation Result\")\n",
        "plt.show()\n",
        " \n",
        "# plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbdMh8pjUmq7"
      },
      "source": [
        "# set width of bar\n",
        "plotData = {'Precision': 0.839779005524862, 'Recall':0.6551724137931034, 'MicroF': 0.7360774818401937}\n",
        " \n",
        "#make list for x and y axis\n",
        "evalMetrices = list(plotData.keys())\n",
        "evalValues = list(plotData.values())\n",
        " \n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "plt.bar(evalMetrices, evalValues, color ='maroon',\n",
        "        width = 0.4)\n",
        "\n",
        "plt.xlabel(\"Evaluation Metrices\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.title(\"Model Evaluation Result\")\n",
        "plt.show()\n",
        " \n",
        "# plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOmGK33GpPkS"
      },
      "source": [
        "## Restore Test Data to Training Set and Train Model on Full Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFLjE4aypZlI"
      },
      "source": [
        "#Upon attempting training, there was not enough RAM/it was too slow to process all texts in their entirities.\n",
        "#As such, following script is used to compress the training data.  It breaks the publications up into sentences.  It then picks,\n",
        "# from each publication, one sentence that contains the data set name inside it, and one sentence that does not.\n",
        "\n",
        "#set our custom entity label to be trained on later\n",
        "LABEL = 'data'\n",
        "\n",
        "TRAIN_DATA = []\n",
        "\n",
        "#loop through tuples in original data\n",
        "#hit_count and miss_count serve as counters for how many sentences we have already extracted from this particular paper\n",
        "for paper in DATA:\n",
        "  hit_count = 0\n",
        "  miss_count = 0\n",
        "\n",
        "  #determine if the paper we are looking at contains any entities\n",
        "  entities = paper[1]['entities']\n",
        "  if len(entities) > 0:\n",
        "    #store the dataset name for this publication\n",
        "    dataset = paper[0][entities[0][0]:entities[0][1]-1]\n",
        "\n",
        "  #split data into sentences, approximately (based on the '.' puncutation mark)\n",
        "  sentences = paper[0].split('.')\n",
        "\n",
        "  #loop through sentences\n",
        "  for sentence in sentences:\n",
        "    this_ent_dict = {\"entities\": []}\n",
        "    if dataset in sentence:\n",
        "      hit_count +=1\n",
        "\n",
        "      #find all occurences of explicit mentions of the dataset in this sentence\n",
        "      for m in re.finditer(dataset,sentence):\n",
        "        this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "      #compressed data: append a tuple of just the sentence (as opposed to the entire paper) containing the dataset entity\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "    \n",
        "    #after find one sentence containing a dataset, append two that do not contain it\n",
        "    elif hit_count > 0 and miss_count <= hit_count:\n",
        "      miss_count += 1\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "\n",
        "    #when have two sentences from this paper (one containing dataset and one without), move on to next paper\n",
        "    if hit_count >= 1 and miss_count >= 2:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8EBChMkqSQ4"
      },
      "source": [
        "#number of entries ((sentence, entity-locations) tuples) in the compressed dataset\n",
        "print(\"number of (sentence, datset_location) tuples in the compressed data: \", len(TRAIN_DATA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjZ7BlfFqjZ2"
      },
      "source": [
        "#find and remove all sentences containing no chars (ex: \"\", \" \", \"      \") as these cause an error in the SpaCy training algorithm\n",
        "\n",
        "count = 0\n",
        "sen = 0\n",
        "while sen < len(TRAIN_DATA):\n",
        "  #if the sentence contains some chars, move on to the next sentence\n",
        "  if TRAIN_DATA[sen][0].strip():\n",
        "      sen += 1\n",
        "  \n",
        "  #if it is a blank sentence, delete it\n",
        "  else:\n",
        "      # print(TRAIN_DATA[sen])\n",
        "      del TRAIN_DATA[sen]\n",
        "      count+=1\n",
        "\n",
        "\n",
        "print(\"Number of blank sentences we removed: \", count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSEM8c4Vqs3m"
      },
      "source": [
        "#script derived from https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
        "\n",
        "#create empty model\n",
        "nlp = sp.blank('en')\n",
        "print(\"created blank english model\")\n",
        "\n",
        "#add ner (entity recognizer) to the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "#add our custom entity label to the entity recognizer\n",
        "ner.add_label(LABEL)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "#set number of epochs we want our model to perform\n",
        "n_iter = 10\n",
        "\n",
        "#make sure we only train our ner pipe to speed up processing\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    for itn in range(n_iter):\n",
        "        print(itn)\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        #split data into batches of 500 sentences\n",
        "        batches = minibatch(TRAIN_DATA, size=500)\n",
        "        for batch in batches:\n",
        "            print('batch size', len(batch))\n",
        "            #extract the sentence and the entities it contains for each sentence\n",
        "            texts, annotations = zip(*batch)\n",
        "            #update the weights\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
        "                        losses=losses)\n",
        "        #loss after this epoch\n",
        "        print('Losses', losses)\n",
        "\n",
        "\n",
        "# Save model \n",
        "output_dir = Path('./models/spacy_full')\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir()\n",
        "nlp.meta['name'] = 'SpaCy_Full'  # rename model\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYG4sLDGwtGi"
      },
      "source": [
        "## Make Predictions on Kaggle Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLYbbBedwwW2"
      },
      "source": [
        "#load text publications\n",
        "import os\n",
        "\n",
        "directory = './data/test/'\n",
        "\n",
        "#this list used to store test data text and id numbers\n",
        "SPACY_TEST = []\n",
        "\n",
        "#loop through publications in directory housing test data\n",
        "for file in os.listdir(directory):\n",
        "    this_pub = \"\"\n",
        "    #path to file\n",
        "    this_pub_id = os.path.join(directory, file)\n",
        "    #load json file\n",
        "    this_pub_json = json.load(open(this_pub_id))\n",
        "    #append all text from the publication in to one (super-long!) string\n",
        "    for section in this_pub_json:\n",
        "        this_pub += section['text']\n",
        "    #get the text id\n",
        "    id = file.split('.')[0]\n",
        "    print(id)\n",
        "    #append a tuple of the text and its id\n",
        "    SPACY_TEST.append((this_pub,id))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TdL6B1_1RQj"
      },
      "source": [
        "#Load SpaCy ner model trained on full training data\n",
        "\n",
        "output_dir = Path('./models/spacy_full')\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp3 = sp.load(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJdovEPd1XdJ"
      },
      "source": [
        "#function provided on Kaggle competition to clean dataset label for submission\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
        "\n",
        "#store predictions in a list\n",
        "predictions = []\n",
        "#loop through test set publications\n",
        "for pub in SPACY_TEST:\n",
        "    this_pred = []\n",
        "    #break up test set into sentences\n",
        "    sentences = pub[0].split('.')\n",
        "    #make predicitons on each sentence\n",
        "    for sentence in sentences:\n",
        "        doc = nlp3(sentence)\n",
        "        #loop through entities found in this sentence, if they contain chars store them, make sure to only store once\n",
        "        for ent in doc.ents:\n",
        "            if len(ent.text) > 0:\n",
        "                this_ent = clean_text(ent.text)\n",
        "                #remove any trailing or leading white spaces\n",
        "                this_ent = this_ent.strip()\n",
        "                if this_ent not in this_pred:\n",
        "                    this_pred.append(this_ent)\n",
        "    #clean for Kaggle submission, concatenates multiple predictions into one string separated by a pipe ('|') deliminator\n",
        "    for pred in range(1, len(this_pred)):\n",
        "        this_pred[0] += '|' + this_pred[pred]\n",
        "    #if no entities predicted, add empty string so length still matches up with number of publications\n",
        "    if len(this_pred) == 0:\n",
        "        this_pred.append(\"\")\n",
        "    predictions.append([this_pred[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNE8RKoF2TGS"
      },
      "source": [
        "#write predictions in appropriate submission format to a .csv file\n",
        "#source: https://realpython.com/python-csv/\n",
        "import csv\n",
        "with open('submission.csv', mode='w') as sub:\n",
        "    sub = csv.writer(sub, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    #header row\n",
        "    sub.writerow(['Id','PredictionString'])\n",
        "    \n",
        "    #for each prediction, format it to the two-column format required for Kaggle submission\n",
        "    for p in range(0, len(predictions)):\n",
        "        sub.writerow([SPACY_TEST[p][1],predictions[p][0]])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}