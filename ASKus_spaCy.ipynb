{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASKus.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FZUIqJj0TLx"
      },
      "source": [
        "# SpaCy Named Entity Recognition Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_une_bE0aS3"
      },
      "source": [
        "## (Professor/TA skip) Mount drive with data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxRaz1CSRANm"
      },
      "source": [
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEBFf0C0RQS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a680b5-ffb6-4262-90f1-a1fd58519e00"
      },
      "source": [
        "#AT/TA skip\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfCXaaGFRXQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec05b7c4-7a1c-4a5b-a34e-834adca8cfba"
      },
      "source": [
        "#AT/TA skip\n",
        "%cd  /content/drive/Shared\\ drives/256/ASKus"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/256/ASKus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGboZMKz0pOB"
      },
      "source": [
        "## Data Exploration and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXe22Rkp0yuw"
      },
      "source": [
        "### Load train.csv containing matrix with paper id's corresponding to dataset label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKYBGIhzgC24"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import fileinput\n",
        "import time\n",
        "\n",
        "import ast\n",
        "\n",
        "import re\n",
        "\n",
        "import spacy as sp\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "import random\n",
        "\n",
        "import json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYY3DmMQpvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "c17e41e6-a60e-41bb-a6b8-a639dad57a3c"
      },
      "source": [
        "\n",
        "\n",
        "train = pd.read_csv('./data/train.csv')\n",
        "train"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n",
              "      <td>The Impact of Dual Enrollment on College Degre...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n",
              "      <td>Educational Attainment of High School Dropouts...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n",
              "      <td>Differences in Outcomes for Female and Male St...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n",
              "      <td>Stepping Stone and Option Value in a Model of ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
              "      <td>Parental Effort, School Resources, and Student...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19656</th>\n",
              "      <td>b3498176-8832-4033-aea6-b5ea85ea04c4</td>\n",
              "      <td>RSNA International Trends: A Global Perspectiv...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "      <td>RSNA International COVID Open Radiology Database</td>\n",
              "      <td>rsna international covid open radiology database</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19657</th>\n",
              "      <td>f77eb51f-c3ac-420b-9586-cb187849c321</td>\n",
              "      <td>MCCS: a novel recognition pattern-based method...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19658</th>\n",
              "      <td>ab59bcdd-7b7c-4107-93f5-0ccaf749236c</td>\n",
              "      <td>Quantitative Structure–Activity Relationship M...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19659</th>\n",
              "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
              "      <td>A ligand-based computational drug repurposing ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19660</th>\n",
              "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
              "      <td>A ligand-based computational drug repurposing ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds data</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19661 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Id  ...                                      cleaned_label\n",
              "0      d0fa7568-7d8e-4db9-870f-f9c6f668c17b  ...              national education longitudinal study\n",
              "1      2f26f645-3dec-485d-b68d-f013c9e05e60  ...              national education longitudinal study\n",
              "2      c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29  ...              national education longitudinal study\n",
              "3      5c9a3bc9-41ba-4574-ad71-e25c1442c8af  ...              national education longitudinal study\n",
              "4      c754dec7-c5a3-4337-9892-c02158475064  ...              national education longitudinal study\n",
              "...                                     ...  ...                                                ...\n",
              "19656  b3498176-8832-4033-aea6-b5ea85ea04c4  ...   rsna international covid open radiology database\n",
              "19657  f77eb51f-c3ac-420b-9586-cb187849c321  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19658  ab59bcdd-7b7c-4107-93f5-0ccaf749236c  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19659  fd23e7e0-a5d2-4f98-992d-9209c85153bb  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19660  fd23e7e0-a5d2-4f98-992d-9209c85153bb  ...    cas covid 19 antiviral candidate compounds data\n",
              "\n",
              "[19661 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAR1lo__RQiU"
      },
      "source": [
        "##script used to take json text files and train.csv and convert them to SpaCy friendly format (takes approx. 80 minutes to complete)\n",
        "\n",
        "##create array to store tuples of publication text and entity (dataset name) locations (this is how SpaCy wants it)\n",
        "# SPACY_TRAIN = []\n",
        "\n",
        "##counter used to track progress\n",
        "# count = 0\n",
        "\n",
        "##loop through train.csv rows\n",
        "# for index, row in train.iterrows():\n",
        "#   print(count)\n",
        "#   this_pub = \"\"\n",
        "\n",
        "##load json file corresponding to publication id in the train.csv row, store known dataset label\n",
        "#   this_pub_id = './data/train/' + row['Id'] + '.json'\n",
        "#   this_data_lab = row['dataset_label']\n",
        "#   this_pub_json = json.load(open(this_pub_id))\n",
        "\n",
        "##append all text from the publication in to one (super-long!) string\n",
        "#   for section in this_pub_json:\n",
        "#     this_pub += section['text']\n",
        "\n",
        "##Find locations of dataset in publications and store in an entity dictionary\n",
        "#   this_ent_dict = {\"entities\": []}\n",
        "#   for m in re.finditer(this_data_lab, this_pub):\n",
        "#     this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "\n",
        "## append the tuple of the entire publication text and the entity locations in the text\n",
        "#   SPACY_TRAIN.append((this_pub, this_ent_dict))\n",
        "#   count+=1\n",
        "\n",
        "# SPACY_TRAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u4Xm-duW8pX"
      },
      "source": [
        "#write SpaCy friendly training data into a text file\n",
        "\n",
        "# file = open(\"./data/spacy/spacy_train.txt\", \"w\")\n",
        "# SPACY_TRAIN_File = repr(SPACY_TRAIN)\n",
        "# file.write(SPACY_TRAIN_File)\n",
        "# file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpq9Wa3Gt1Cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365be225-2897-46d7-ad39-b199ff4a16b2"
      },
      "source": [
        "#script to read in SpaCy friendly data from text file (script modified from one here: https://www.geeksforgeeks.org/how-to-read-large-text-files-in-python/)\n",
        "  \n",
        "#time at the start of program is noted\n",
        "start = time.time()\n",
        "spacy_train = \"\"\n",
        "#keeps a track of number of lines in the file\n",
        "count = 0\n",
        "for lines in fileinput.input([\"./data/spacy/spacy_train.txt\"]):\n",
        "    spacy_train = lines\n",
        "    count = count + 1\n",
        "      \n",
        "#time at the end of program execution is noted\n",
        "end = time.time()\n",
        "  \n",
        "#total time taken to print the file\n",
        "print(\"Execution time in seconds: \",(end - start))\n",
        "print(\"No. of lines printed: \",count)\n",
        "\n",
        "#number of chars in file (exceeds one billion)\n",
        "print(\"Number of characters in the data file (too many): \", len(spacy_train))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execution time in seconds:  12.284823417663574\n",
            "No. of lines printed:  1\n",
            "Number of characters in the data file (too many):  1093591763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYMJcTI7NdDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bb593b-937e-4764-c0e9-0b8b3242693c"
      },
      "source": [
        "#SpaCy friendly training data was read in from .txt file as a string - use following to convert back to list of tuples\n",
        "#Source code: https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list\n",
        "\n",
        "DATA = ast.literal_eval(spacy_train)\n",
        "\n",
        "print(\"number of (paper, dataset_location) tuples in the data: \", len(DATA))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of (paper, dataset_location) tuples in the data:  19661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npbPApGyIJVe"
      },
      "source": [
        "#split data into 70% training and 30% test\n",
        "#Train only on the 70% and keep the rest for evaluation purposes\n",
        "#As model training and internal evaluation was done over multiple sessions, this was run once and the separate\n",
        "# train and test data were saved to our shared Google Drive to maintain the integrity of our evaluation results,\n",
        "# hence it is commented out here \n",
        "\n",
        "# #randomly shuffle the tuples\n",
        "# random.shuffle(DATA)\n",
        "\n",
        "# train_mark = (len(DATA)*7//10)\n",
        "# TRAIN_FULL = DATA[:train_mark]\n",
        "# TEST_FULL = DATA[train_mark:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQJM72AqAWrr"
      },
      "source": [
        "# write train/test split into a text file, as model will be trained on this split\n",
        "\n",
        "#As model training and internal evaluation was done over multiple sessions, this was run once and the separate\n",
        "# train and test data were saved to our shared Google Drive to maintain the integrity of our evaluation results,\n",
        "# hence it is commented out here \n",
        "\n",
        "# file = open(\"./data/spacy/TRAIN_FULL.txt\", \"w\")\n",
        "# TRAIN_FULL_SET = repr(TRAIN_FULL)\n",
        "# file.write(TRAIN_FULL_SET)\n",
        "# file.close()\n",
        "\n",
        "# file = open(\"./data/spacy/TEST_FULL.txt\", \"w\")\n",
        "# TEST_FULL_SET = repr(TEST_FULL)\n",
        "# file.write(TEST_FULL_SET)\n",
        "# file.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svLcu6si-FCA"
      },
      "source": [
        "#Upon attempting training, there was not enough RAM/it was too slow to process all texts in their entirities.\n",
        "#As such, following script is used to compress the training data.  It breaks the publications up into sentences.  It then picks,\n",
        "# from each publication, one sentence that contains the data set name inside it, and one sentence that does not.\n",
        "\n",
        "#set our custom entity label to be trained on later\n",
        "LABEL = 'data'\n",
        "\n",
        "TRAIN_DATA = []\n",
        "\n",
        "#loop through tuples in original data\n",
        "#hit_count and miss_count serve as counters for how many sentences we have already extracted from this particular paper\n",
        "for paper in TRAIN_FULL:\n",
        "  hit_count = 0\n",
        "  miss_count = 0\n",
        "\n",
        "  #determine if the paper we are looking at contains any entities\n",
        "  entities = paper[1]['entities']\n",
        "  if len(entities) > 0:\n",
        "    #store the dataset name for this publication\n",
        "    dataset = paper[0][entities[0][0]:entities[0][1]-1]\n",
        "\n",
        "  #split data into sentences, approximately (based on the '.' puncutation mark)\n",
        "  sentences = paper[0].split('.')\n",
        "\n",
        "  #loop through sentences\n",
        "  for sentence in sentences:\n",
        "    this_ent_dict = {\"entities\": []}\n",
        "    if dataset in sentence:\n",
        "      hit_count +=1\n",
        "\n",
        "      #find all occurences of explicit mentions of the dataset in this sentence\n",
        "      for m in re.finditer(dataset,sentence):\n",
        "        this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "      #compressed data: append a tuple of just the sentence (as opposed to the entire paper) containing the dataset entity\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "    \n",
        "    #after find one sentence containing a dataset, append two that do not contain it\n",
        "    elif hit_count > 0 and miss_count <= hit_count:\n",
        "      miss_count += 1\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "\n",
        "    #when have two sentences from this paper (one containing dataset and one without), move on to next paper\n",
        "    if hit_count >= 1 and miss_count >= 2:\n",
        "      break"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drMyeKVVQeJA"
      },
      "source": [
        "#number of entries ((sentence, entity-locations) tuples) in the compressed dataset\n",
        "print(\"number of (sentence, datset_location) tuples in the compressed data: \", len(TRAIN_DATA))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bS0eO6WFv1h",
        "outputId": "238ad424-9d75-43fc-a611-d9a0cabad894"
      },
      "source": [
        "#find and remove all sentences containing no chars (ex: \"\", \" \", \"      \") as these cause an error in the SpaCy training algorithm\n",
        "\n",
        "count = 0\n",
        "sen = 0\n",
        "while sen < len(TRAIN_DATA):\n",
        "  #if the sentence contains some chars, move on to the next sentence\n",
        "  if TRAIN_DATA[sen][0].strip():\n",
        "      sen += 1\n",
        "  \n",
        "  #if it is a blank sentence, delete it\n",
        "  else:\n",
        "      # print(TRAIN_DATA[sen])\n",
        "      del TRAIN_DATA[sen]\n",
        "      count+=1\n",
        "\n",
        "\n",
        "print(\"Number of blank sentences we removed: \", count)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of blank sentences we removed:  52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgw5tlXP7rN7"
      },
      "source": [
        "## Training SpaCy NER with Custom Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jo4O0XQYzbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58de194-5d6c-4207-d3e4-ca67cbae008e"
      },
      "source": [
        "#script derived from https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
        "\n",
        "\n",
        "#create empty model\n",
        "nlp = sp.blank('en')\n",
        "print(\"created blank english model\")\n",
        "\n",
        "#add ner (entity recognizer) to the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "#add our custom entity label to the entity recognizer\n",
        "ner.add_label(LABEL)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "#set number of epochs we want our model to perform\n",
        "n_iter = 10\n",
        "\n",
        "#make sure we only train our ner pipe to speed up processing\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    for itn in range(n_iter):\n",
        "        print(itn)\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        #split data into batches of 500 sentences\n",
        "        batches = minibatch(TRAIN_DATA, size=500)\n",
        "        for batch in batches:\n",
        "            print('batch size', len(batch))\n",
        "            #extract the sentence and the entities it contains for each sentence\n",
        "            texts, annotations = zip(*batch)\n",
        "            #update the weights\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
        "                        losses=losses)\n",
        "        #loss after this epoch\n",
        "        print('Losses', losses)\n",
        "\n",
        "\n",
        "# Save model \n",
        "output_dir = Path('./models/spacy')\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir()\n",
        "nlp.meta['name'] = 'SpaCy'  # rename model\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created blank english model\n",
            "0\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 65895.00741767883}\n",
            "1\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 3386.1671180725098}\n",
            "2\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 2294.766040354967}\n",
            "3\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 1955.0614629685879}\n",
            "4\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 1565.0468806996942}\n",
            "5\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 1504.1451077274978}\n",
            "6\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 1194.790364574641}\n",
            "7\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 1095.5214811284095}\n",
            "8\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 1014.7345414981246}\n",
            "9\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 445\n",
            "Losses {'ner': 954.4369266508147}\n",
            "Saved model to models/spacy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQYAyPPaEP9j"
      },
      "source": [
        "## Internal Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89iUoceWfwio",
        "outputId": "e634ac70-3a7b-47b5-ee04-0349027c8194"
      },
      "source": [
        "#Load \"test\" set (30% training data held aside above)\n",
        "#Note: these are not compressed into sentences, it is full texts.  The evaluation script below breaks into sentences\n",
        "\n",
        "#script to read in SpaCy friendly data from text file (script modified from one here: https://www.geeksforgeeks.org/how-to-read-large-text-files-in-python/)\n",
        "  \n",
        "#time at the start of program is noted\n",
        "start = time.time()\n",
        "TEST_FULL = \"\"\n",
        "#keeps a track of number of lines in the file\n",
        "count = 0\n",
        "for lines in fileinput.input([\"./data/spacy/TEST_FULL.txt\"]):\n",
        "    TEST_FULL = lines\n",
        "    count = count + 1\n",
        "      \n",
        "#time at the end of program execution is noted\n",
        "end = time.time()\n",
        "  \n",
        "#total time taken to print the file\n",
        "print(\"Execution time in seconds: \",(end - start))\n",
        "print(\"No. of lines printed: \",count)\n",
        "\n",
        "#number of chars in file (exceeds one billion)\n",
        "print(\"Number of characters in the data file : \", len(TEST_FULL))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execution time in seconds:  1.1361868381500244\n",
            "No. of lines printed:  1\n",
            "Number of characters in the data file :  328595605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMalJFigg4CY",
        "outputId": "f010a571-7784-471d-fee5-919bc18e6190"
      },
      "source": [
        "#Test data was read in from .txt file as a string - use following to convert back to list of tuples\n",
        "#Source code: https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list\n",
        "\n",
        "TEST_FULL = ast.literal_eval(TEST_FULL)\n",
        "\n",
        "print(\"number of (paper, dataset_location) tuples in the data: \", len(TEST_FULL))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of (paper, dataset_location) tuples in the data:  5899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcSC--8rRGmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f965d645-f329-4118-c87a-a7968e1eea8d"
      },
      "source": [
        "#Load SpaCy ner model trained on 70% of the data, evaluate on 1% unseen test data (1% (60 full papers) used out of 30% (~6000 full papers) due to runtime constraints)\n",
        "\n",
        "output_dir = Path('./models/spacy')\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = sp.load(output_dir)\n",
        "\n",
        "#function provided on Kaggle competition to clean dataset label for submission\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
        "\n",
        "#lists to hold actual dataset labels and predicted dataset labels across papers\n",
        "ground_truth = []\n",
        "predictions = []\n",
        "\n",
        "#counter used to track progress of the script\n",
        "count = 0\n",
        "\n",
        "#We are taking 1% of the test data to evaluate on.  This is approx. 60 papers.  \n",
        "# We chose this for a couple reasons.  1) Kaggle only asks us to submit predictions for 4 papers, 2) predicting on 60 papers vs. 6000 papers reduces runtime\n",
        "# from ~730 minutes to ~30 minutes.\n",
        "\n",
        "#randomly shuffle the tuples\n",
        "random.shuffle(TEST_FULL)\n",
        "\n",
        "#pick 60 papers at random from test set\n",
        "start_range = random.randint(0, 5839)\n",
        "end_range = start_range + 60\n",
        "print('start_range', start_range)\n",
        "print('end_range', end_range)\n",
        "\n",
        "#The following loop creates two arrays, one with the actual dataset labels for each paper and one for the predicted labels.\n",
        "# The order of each list matches with each other, which makes comparison easy later on.\n",
        "\n",
        "#Loop through each (paper, dataset labels) tuple\n",
        "for test_text in TEST_FULL[start_range:end_range]:\n",
        "    count +=1\n",
        "    print(count)\n",
        "\n",
        "    #following list stores actual entities for the current paper\n",
        "    this_gt = []\n",
        "\n",
        "    #loop through entities for this paper, store once\n",
        "    for gt_ent in test_text[1]['entities']:\n",
        "        dataset = test_text[0][gt_ent[0]:gt_ent[1]-1]\n",
        "        dataset = clean_text(dataset)\n",
        "        if dataset not in this_gt:\n",
        "            this_gt.append(dataset)\n",
        "    ground_truth.append(this_gt)\n",
        "    \n",
        "    #this_pred stores all dataset label predictions for this paper\n",
        "    this_pred = []\n",
        "\n",
        "    #split current paper into sentences, approximately\n",
        "    sentences = test_text[0].split('.')\n",
        "\n",
        "    #for each sentence, predict dataset labels\n",
        "    for sentence in sentences:\n",
        "        doc = nlp2(sentence)\n",
        "        #loop through entities found in this sentence, if they contain chars store them, make sure to only store once\n",
        "        for ent in doc.ents:\n",
        "            if len(ent.text) > 0:\n",
        "                this_ent = clean_text(ent.text)\n",
        "                #remove any trailing or leading white spaces\n",
        "                this_ent = this_ent.strip()\n",
        "                if this_ent not in this_pred:\n",
        "                    this_pred.append(this_ent)\n",
        "                \n",
        "    predictions.append(this_pred)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from models/spacy\n",
            "start_range 2502\n",
            "end_range 2562\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kptqWG9S3Pc"
      },
      "source": [
        "#initialize variables to hold true positives (TP), false negatives (FN), and false positives (FP) (defined below)\n",
        "TP=0\n",
        "FP=0\n",
        "FN=0\n",
        "\n",
        "#Kaggle-provided Jaccard similarity function between strings\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "#loop through the range of both arrays (ground_truth and prediction arrays are the same length)\n",
        "for num in range(0, len(ground_truth)):\n",
        "    this_gt_set = ground_truth[num]\n",
        "\n",
        "    #Kaggle competition eval. page says: \"Predicted strings for each publication are sorted alphabetically and processed in that order. \n",
        "    #Any scoring ties are resolved on the basis of that sort.\" Hence we sort the prediction strings alphabetically\n",
        "    this_pred_set = sorted(predictions[num])\n",
        "\n",
        "    #keep track of which predictions were matched, unmatched predictions count as FP\n",
        "    matched_pred = []\n",
        "\n",
        "    #loop through ground truth predictions for this paper\n",
        "    for key in this_gt_set:\n",
        "        #max_sim keeps track of most simliar prediction to ground truth string\n",
        "        max_sim = 0\n",
        "        #loop through this papers predictions\n",
        "        for pred in this_pred_set:\n",
        "            this_sim = jaccard(key, pred)\n",
        "            #store prediction (mark as matched) if it has greater than 0 jaccard similarity to some ground truth dataset label\n",
        "            if this_sim > 0:\n",
        "                if pred not in matched_pred:\n",
        "                    matched_pred.append(pred)\n",
        "            if this_sim > max_sim:\n",
        "                max_sim = this_sim\n",
        "        #if a ground truth is predicted with greater than 0.5 similarity, mark as TP\n",
        "        if max_sim >= 0.5:\n",
        "            TP += 1\n",
        "        #if ground truth is only predicted with greater than 0 similarity but less than 0.5, count as FP\n",
        "        elif max_sim < 0.5 and max_sim > 0:\n",
        "            FP += 1\n",
        "        #if ground truth is not matched by any predictions, count as FN\n",
        "        elif max_sim == 0:\n",
        "            FN += 1\n",
        "        #all unmatched predicitons counted as FP\n",
        "        FP += len(this_pred_set) - len(matched_pred)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbdMh8pjUmq7",
        "outputId": "c4c9102c-26c6-4b92-e762-fefd44d0f295"
      },
      "source": [
        "#Calculate precision, recall, and micro F0.5 scores\n",
        "precision = TP/(TP+FP)\n",
        "print(\"Precision: \", precision)\n",
        "\n",
        "recall = TP/(TP+FN)\n",
        "print(\"Recall: \", recall)\n",
        "\n",
        "microF = (2*precision*recall)/(precision+recall)\n",
        "print(\"Micro F0.5: \", microF)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  0.8297872340425532\n",
            "Recall:  0.75\n",
            "Micro F0.5:  0.787878787878788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOmGK33GpPkS"
      },
      "source": [
        "## Restore Test Data to Training Set and Train Model on Full Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFLjE4aypZlI"
      },
      "source": [
        "#Upon attempting training, there was not enough RAM/it was too slow to process all texts in their entirities.\n",
        "#As such, following script is used to compress the training data.  It breaks the publications up into sentences.  It then picks,\n",
        "# from each publication, one sentence that contains the data set name inside it, and one sentence that does not.\n",
        "\n",
        "#set our custom entity label to be trained on later\n",
        "LABEL = 'data'\n",
        "\n",
        "TRAIN_DATA = []\n",
        "\n",
        "#loop through tuples in original data\n",
        "#hit_count and miss_count serve as counters for how many sentences we have already extracted from this particular paper\n",
        "for paper in DATA:\n",
        "  hit_count = 0\n",
        "  miss_count = 0\n",
        "\n",
        "  #determine if the paper we are looking at contains any entities\n",
        "  entities = paper[1]['entities']\n",
        "  if len(entities) > 0:\n",
        "    #store the dataset name for this publication\n",
        "    dataset = paper[0][entities[0][0]:entities[0][1]-1]\n",
        "\n",
        "  #split data into sentences, approximately (based on the '.' puncutation mark)\n",
        "  sentences = paper[0].split('.')\n",
        "\n",
        "  #loop through sentences\n",
        "  for sentence in sentences:\n",
        "    this_ent_dict = {\"entities\": []}\n",
        "    if dataset in sentence:\n",
        "      hit_count +=1\n",
        "\n",
        "      #find all occurences of explicit mentions of the dataset in this sentence\n",
        "      for m in re.finditer(dataset,sentence):\n",
        "        this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "      #compressed data: append a tuple of just the sentence (as opposed to the entire paper) containing the dataset entity\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "    \n",
        "    #after find one sentence containing a dataset, append two that do not contain it\n",
        "    elif hit_count > 0 and miss_count <= hit_count:\n",
        "      miss_count += 1\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "\n",
        "    #when have two sentences from this paper (one containing dataset and one without), move on to next paper\n",
        "    if hit_count >= 1 and miss_count >= 2:\n",
        "      break"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8EBChMkqSQ4",
        "outputId": "2f2bc6e4-10f8-41d6-a2ad-06c127dbdf6d"
      },
      "source": [
        "#number of entries ((sentence, entity-locations) tuples) in the compressed dataset\n",
        "print(\"number of (sentence, datset_location) tuples in the compressed data: \", len(TRAIN_DATA))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of (sentence, datset_location) tuples in the compressed data:  47600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjZ7BlfFqjZ2",
        "outputId": "9d7344df-b98f-471f-e9df-27eb4cef3ef4"
      },
      "source": [
        "#find and remove all sentences containing no chars (ex: \"\", \" \", \"      \") as these cause an error in the SpaCy training algorithm\n",
        "\n",
        "count = 0\n",
        "sen = 0\n",
        "while sen < len(TRAIN_DATA):\n",
        "  #if the sentence contains some chars, move on to the next sentence\n",
        "  if TRAIN_DATA[sen][0].strip():\n",
        "      sen += 1\n",
        "  \n",
        "  #if it is a blank sentence, delete it\n",
        "  else:\n",
        "      # print(TRAIN_DATA[sen])\n",
        "      del TRAIN_DATA[sen]\n",
        "      count+=1\n",
        "\n",
        "\n",
        "print(\"Number of blank sentences we removed: \", count)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of blank sentences we removed:  85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSEM8c4Vqs3m",
        "outputId": "66bf1eac-0745-4bc6-93c1-1ee16c96438e"
      },
      "source": [
        "#script derived from https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
        "\n",
        "#create empty model\n",
        "nlp = sp.blank('en')\n",
        "print(\"created blank english model\")\n",
        "\n",
        "#add ner (entity recognizer) to the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "#add our custom entity label to the entity recognizer\n",
        "ner.add_label(LABEL)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "#set number of epochs we want our model to perform\n",
        "n_iter = 10\n",
        "\n",
        "#make sure we only train our ner pipe to speed up processing\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    for itn in range(n_iter):\n",
        "        print(itn)\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        #split data into batches of 500 sentences\n",
        "        batches = minibatch(TRAIN_DATA, size=500)\n",
        "        for batch in batches:\n",
        "            print('batch size', len(batch))\n",
        "            #extract the sentence and the entities it contains for each sentence\n",
        "            texts, annotations = zip(*batch)\n",
        "            #update the weights\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
        "                        losses=losses)\n",
        "        #loss after this epoch\n",
        "        print('Losses', losses)\n",
        "\n",
        "\n",
        "# Save model \n",
        "output_dir = Path('./models/spacy_full')\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir()\n",
        "nlp.meta['name'] = 'SpaCy_Full'  # rename model\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created blank english model\n",
            "0\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 66355.59860564083}\n",
            "1\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 3567.0025023753496}\n",
            "2\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 2699.7429987216765}\n",
            "3\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 2311.7786699546373}\n",
            "4\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 1789.5117780524633}\n",
            "5\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 1610.8652390786447}\n",
            "6\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 1335.9672588747194}\n",
            "7\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 1321.408370881156}\n",
            "8\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 1188.7145780364287}\n",
            "9\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 15\n",
            "Losses {'ner': 1215.8606668129487}\n",
            "Saved model to models/spacy_full\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYG4sLDGwtGi"
      },
      "source": [
        "## Make Predictions on Kaggle Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLYbbBedwwW2",
        "outputId": "bd5808e8-fa4c-4d97-8d48-c5fb04af05ba"
      },
      "source": [
        "#load text publications\n",
        "import os\n",
        "\n",
        "directory = './data/test/'\n",
        "\n",
        "#this list used to store test data text and id numbers\n",
        "SPACY_TEST = []\n",
        "\n",
        "#loop through publications in directory housing test data\n",
        "for file in os.listdir(directory):\n",
        "    this_pub = \"\"\n",
        "    #path to file\n",
        "    this_pub_id = os.path.join(directory, file)\n",
        "    #load json file\n",
        "    this_pub_json = json.load(open(this_pub_id))\n",
        "    #append all text from the publication in to one (super-long!) string\n",
        "    for section in this_pub_json:\n",
        "        this_pub += section['text']\n",
        "    #get the text id\n",
        "    id = file.split('.')[0]\n",
        "    print(id)\n",
        "    #append a tuple of the text and its id\n",
        "    SPACY_TEST.append((this_pub,id))\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2100032a-7c33-4bff-97ef-690822c43466\n",
            "2f392438-e215-4169-bebf-21ac4ff253e1\n",
            "3f316b38-1a24-45a9-8d8c-4e05a42257c6\n",
            "8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TdL6B1_1RQj",
        "outputId": "c012958a-5637-4b53-8f11-65ddae9e992d"
      },
      "source": [
        "#Load SpaCy ner model trained on full training data\n",
        "\n",
        "output_dir = Path('./models/spacy_full')\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp3 = sp.load(output_dir)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from models/spacy_full\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJdovEPd1XdJ"
      },
      "source": [
        "#function provided on Kaggle competition to clean dataset label for submission\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
        "\n",
        "#store predictions in a list\n",
        "predictions = []\n",
        "#loop through test set publications\n",
        "for pub in SPACY_TEST:\n",
        "    this_pred = []\n",
        "    #break up test set into sentences\n",
        "    sentences = pub[0].split('.')\n",
        "    #make predicitons on each sentence\n",
        "    for sentence in sentences:\n",
        "        doc = nlp3(sentence)\n",
        "        #loop through entities found in this sentence, if they contain chars store them, make sure to only store once\n",
        "        for ent in doc.ents:\n",
        "            if len(ent.text) > 0:\n",
        "                this_ent = clean_text(ent.text)\n",
        "                #remove any trailing or leading white spaces\n",
        "                this_ent = this_ent.strip()\n",
        "                if this_ent not in this_pred:\n",
        "                    this_pred.append(this_ent)\n",
        "    #clean for Kaggle submission, concatenates multiple predictions into one string separated by a pipe ('|') deliminator\n",
        "    for pred in range(1, len(this_pred)):\n",
        "        this_pred[0] += '|' + this_pred[pred]\n",
        "    #if no entities predicted, add empty string so length still matches up with number of publications\n",
        "    if len(this_pred) == 0:\n",
        "        this_pred.append(\"\")\n",
        "    predictions.append([this_pred[0]])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNE8RKoF2TGS"
      },
      "source": [
        "#write predictions in appropriate submission format to a .csv file\n",
        "#source: https://realpython.com/python-csv/\n",
        "import csv\n",
        "with open('submission.csv', mode='w') as sub:\n",
        "    sub = csv.writer(sub, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    #header row\n",
        "    sub.writerow(['Id','PredictionString'])\n",
        "    \n",
        "    #for each prediction, format it to the two-column format required for Kaggle submission\n",
        "    for p in range(0, len(predictions)):\n",
        "        sub.writerow([SPACY_TEST[p][1],predictions[p][0]])"
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}