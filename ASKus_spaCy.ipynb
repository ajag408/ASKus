{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASKus.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FZUIqJj0TLx"
      },
      "source": [
        "# SpaCy Named Entity Recognition Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_une_bE0aS3"
      },
      "source": [
        "## (AT/TA skip) Mount drive with data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxRaz1CSRANm"
      },
      "source": [
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEBFf0C0RQS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e58fdde-949d-4e37-9049-d31b50c8df53"
      },
      "source": [
        "#AT/TA skip\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfCXaaGFRXQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e61326-ffaf-4942-b85a-b55a5560dc2f"
      },
      "source": [
        "#AT/TA skip\n",
        "%cd  /content/drive/Shared\\ drives/256/ASKus"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/256/ASKus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGboZMKz0pOB"
      },
      "source": [
        "## Data Exploration and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXe22Rkp0yuw"
      },
      "source": [
        "### Load train.csv containing matrix with paper id's corresponding to dataset label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYY3DmMQpvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "27acf5fd-358c-4b7a-8868-a1889e39a8ed"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('./data/train.csv')\n",
        "train"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n",
              "      <td>The Impact of Dual Enrollment on College Degre...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n",
              "      <td>Educational Attainment of High School Dropouts...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n",
              "      <td>Differences in Outcomes for Female and Male St...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n",
              "      <td>Stepping Stone and Option Value in a Model of ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
              "      <td>Parental Effort, School Resources, and Student...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19656</th>\n",
              "      <td>b3498176-8832-4033-aea6-b5ea85ea04c4</td>\n",
              "      <td>RSNA International Trends: A Global Perspectiv...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "      <td>RSNA International COVID Open Radiology Database</td>\n",
              "      <td>rsna international covid open radiology database</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19657</th>\n",
              "      <td>f77eb51f-c3ac-420b-9586-cb187849c321</td>\n",
              "      <td>MCCS: a novel recognition pattern-based method...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19658</th>\n",
              "      <td>ab59bcdd-7b7c-4107-93f5-0ccaf749236c</td>\n",
              "      <td>Quantitative Structure–Activity Relationship M...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19659</th>\n",
              "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
              "      <td>A ligand-based computational drug repurposing ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19660</th>\n",
              "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
              "      <td>A ligand-based computational drug repurposing ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds data</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds data</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19661 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Id  ...                                      cleaned_label\n",
              "0      d0fa7568-7d8e-4db9-870f-f9c6f668c17b  ...              national education longitudinal study\n",
              "1      2f26f645-3dec-485d-b68d-f013c9e05e60  ...              national education longitudinal study\n",
              "2      c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29  ...              national education longitudinal study\n",
              "3      5c9a3bc9-41ba-4574-ad71-e25c1442c8af  ...              national education longitudinal study\n",
              "4      c754dec7-c5a3-4337-9892-c02158475064  ...              national education longitudinal study\n",
              "...                                     ...  ...                                                ...\n",
              "19656  b3498176-8832-4033-aea6-b5ea85ea04c4  ...   rsna international covid open radiology database\n",
              "19657  f77eb51f-c3ac-420b-9586-cb187849c321  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19658  ab59bcdd-7b7c-4107-93f5-0ccaf749236c  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19659  fd23e7e0-a5d2-4f98-992d-9209c85153bb  ...  cas covid 19 antiviral candidate compounds dat...\n",
              "19660  fd23e7e0-a5d2-4f98-992d-9209c85153bb  ...    cas covid 19 antiviral candidate compounds data\n",
              "\n",
              "[19661 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAR1lo__RQiU"
      },
      "source": [
        "##script used to take json text files and train.csv and convert them to SpaCy friendly format (takes approx. 80 minutes to complete)\n",
        "\n",
        "##create array to store tuples of publication text and entity (dataset name) locations (this is how SpaCy wants it)\n",
        "# SPACY_TRAIN = []\n",
        "\n",
        "##counter used to track progress\n",
        "# count = 0\n",
        "\n",
        "##loop through train.csv rows\n",
        "# for index, row in train.iterrows():\n",
        "#   print(count)\n",
        "#   this_pub = \"\"\n",
        "\n",
        "##load json file corresponding to publication id in the train.csv row, store known dataset label\n",
        "#   this_pub_id = './data/train/' + row['Id'] + '.json'\n",
        "#   this_data_lab = row['dataset_label']\n",
        "#   this_pub_json = json.load(open(this_pub_id))\n",
        "\n",
        "##append all text from the publication in to one (super-long!) string\n",
        "#   for section in this_pub_json:\n",
        "#     this_pub += section['text']\n",
        "\n",
        "##Find locations of dataset in publications and store in an entity dictionary\n",
        "#   this_ent_dict = {\"entities\": []}\n",
        "#   for m in re.finditer(this_data_lab, this_pub):\n",
        "#     this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "\n",
        "## append the tuple of the entire publication text and the entity locations in the text\n",
        "#   SPACY_TRAIN.append((this_pub, this_ent_dict))\n",
        "#   count+=1\n",
        "\n",
        "# SPACY_TRAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u4Xm-duW8pX"
      },
      "source": [
        "#write SpaCy friendly training data into a text file\n",
        "\n",
        "# file = open(\"./data/spacy_train.txt\", \"w\")\n",
        "# SPACY_TRAIN_File = repr(SPACY_TRAIN)\n",
        "# file.write(SPACY_TRAIN_File)\n",
        "# file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpq9Wa3Gt1Cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2723c793-ca74-4e72-f818-9c4739c52dbb"
      },
      "source": [
        "#script to read in SpaCy friendly data from text file (script modified from one here: https://www.geeksforgeeks.org/how-to-read-large-text-files-in-python/)\n",
        "import fileinput\n",
        "import time\n",
        "  \n",
        "#time at the start of program is noted\n",
        "start = time.time()\n",
        "spacy_train = \"\"\n",
        "#keeps a track of number of lines in the file\n",
        "count = 0\n",
        "for lines in fileinput.input([\"./data/spacy_train.txt\"]):\n",
        "    spacy_train = lines\n",
        "    count = count + 1\n",
        "      \n",
        "#time at the end of program execution is noted\n",
        "end = time.time()\n",
        "  \n",
        "#total time taken to print the file\n",
        "print(\"Execution time in seconds: \",(end - start))\n",
        "print(\"No. of lines printed: \",count)\n",
        "\n",
        "#number of chars in file (exceeds one billion)\n",
        "print(\"Number of characters in the data file (too many): \", len(spacy_train))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execution time in seconds:  10.072824001312256\n",
            "No. of lines printed:  1\n",
            "Number of characters in the data file (too many):  1093591763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYMJcTI7NdDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7fd928-dd64-4540-9504-90e15970a9b1"
      },
      "source": [
        "#SpaCy friendly training data was read in from .txt file as a string - use following ton convert back to list of tuples\n",
        "#Source code: https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list\n",
        "import ast\n",
        "\n",
        "DATA = ast.literal_eval(spacy_train)\n",
        "\n",
        "print(\"number of (paper, dataset_location) tuples in the data: \", len(DATA))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of (paper, dataset_location) tuples in the data:  19661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npbPApGyIJVe"
      },
      "source": [
        "#split data into 70% training and 30% test\n",
        "#Train only on the 70% and keep the rest for evaluation purposes\n",
        "\n",
        "import random\n",
        "\n",
        "#randomly shuffle the tuples\n",
        "random.shuffle(DATA)\n",
        "\n",
        "train_mark = (len(DATA)*7//10)\n",
        "TRAIN_FULL = DATA[:train_mark]\n",
        "TEST_FULL = DATA[train_mark:]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svLcu6si-FCA"
      },
      "source": [
        "#Upon attempting training, there was not enough RAM/it was too slow to process all texts in their entirities.\n",
        "#As such, following script is used to compress the training data.  It breaks the publications up into sentences.  It then picks,\n",
        "# from each publication, one sentence that contains the data set name inside it, and one sentence that does not.\n",
        "import re\n",
        "\n",
        "#set our custom entity label to be trained on later\n",
        "LABEL = 'data'\n",
        "\n",
        "TRAIN_DATA = []\n",
        "\n",
        "#loop through tuples in original data\n",
        "#hit_count and miss_count serve as counters for how many sentences we have already extracted from this particular paper\n",
        "for paper in TRAIN_FULL:\n",
        "  hit_count = 0\n",
        "  miss_count = 0\n",
        "\n",
        "  #determine if the paper we are looking at contains any entities\n",
        "  entities = paper[1]['entities']\n",
        "  if len(entities) > 0:\n",
        "    #store the dataset name for this publication\n",
        "    dataset = paper[0][entities[0][0]:entities[0][1]-1]\n",
        "\n",
        "  #split data into sentences, approximately (based on the '.' puncutation mark)\n",
        "  sentences = paper[0].split('.')\n",
        "\n",
        "  #loop through sentences\n",
        "  for sentence in sentences:\n",
        "    this_ent_dict = {\"entities\": []}\n",
        "    if dataset in sentence:\n",
        "      hit_count +=1\n",
        "\n",
        "      #find all occurences of explicit mentions of the dataset in this sentence\n",
        "      for m in re.finditer(dataset,sentence):\n",
        "        this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "      #compressed data: append a tuple of just the sentence (as opposed to the entire paper) containing the dataset entity\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "    \n",
        "    #after find one sentence containing a dataset, append two that do not contain it\n",
        "    elif hit_count > 0 and miss_count <= hit_count:\n",
        "      miss_count += 1\n",
        "      TRAIN_DATA.append((sentence, this_ent_dict))\n",
        "\n",
        "    #when have two sentences from this paper (one containing dataset and one without), move on to next paper\n",
        "    if hit_count >= 1 and miss_count >= 2:\n",
        "      break"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drMyeKVVQeJA",
        "outputId": "08008dc0-be9d-48d1-cf0c-86ee5bbbb130"
      },
      "source": [
        "#number of entries ((sentence, entity-locations) tuples) in the compressed dataset\n",
        "print(\"number of (sentence, datset_location) tuples in the compressed data: \", len(TRAIN_DATA))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of (sentence, datset_location) tuples in the compressed data:  34230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bS0eO6WFv1h",
        "outputId": "cbaa55b1-ea23-4b0a-9d54-7503f4aa16a2"
      },
      "source": [
        "#find and remove all sentences containing no chars (ex: \"\", \" \", \"      \") as these cause an error in the SpaCy training algorithm\n",
        "\n",
        "count = 0\n",
        "sen = 0\n",
        "while sen < len(TRAIN_DATA):\n",
        "  #if the sentence contains some chars, move on to the next sentence\n",
        "  if TRAIN_DATA[sen][0].strip():\n",
        "      sen += 1\n",
        "  \n",
        "  #if it is a blank sentence, delete it\n",
        "  else:\n",
        "      # print(TRAIN_DATA[sen])\n",
        "      del TRAIN_DATA[sen]\n",
        "      count+=1\n",
        "\n",
        "\n",
        "print(\"Number of blank sentences we removed: \", count)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of blank sentences we removed:  72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SFaQORhPz0k"
      },
      "source": [
        "#split data into 70% training and 30% test\n",
        "#Train only on the 70% and keep the rest for evaluation purposes\n",
        "\n",
        "# import random\n",
        "\n",
        "# #randomly shuffle the tuples\n",
        "# random.shuffle(new_spacy)\n",
        "\n",
        "# train_mark = (len(new_spacy)*7//10)\n",
        "# TRAIN_DATA = new_spacy[:train_mark]\n",
        "# TEST_DATA = new_spacy[train_mark:]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgw5tlXP7rN7"
      },
      "source": [
        "## Training SpaCy NER with Custom Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jo4O0XQYzbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90280cf9-624e-48ba-ead5-cbf7b9aab332"
      },
      "source": [
        "#script derived from https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
        "\n",
        "import spacy as sp\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "#create empty model\n",
        "nlp = sp.blank('en')\n",
        "print(\"created blank english model\")\n",
        "\n",
        "#add ner (entity recognizer) to the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "#add our custom entity label to the entity recognizer\n",
        "ner.add_label(LABEL)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "#set number of epochs we want our model to perform\n",
        "n_iter = 3\n",
        "\n",
        "#make sure we only train our ner pipe to speed up processing\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    for itn in range(n_iter):\n",
        "        print(itn)\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        #split data into batches of 500 sentences\n",
        "        batches = minibatch(TRAIN_DATA, size=500)\n",
        "        for batch in batches:\n",
        "            print('batch size', len(batch))\n",
        "            #extract the sentence and the entities it contains for each sentence\n",
        "            texts, annotations = zip(*batch)\n",
        "            #update the weights\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
        "                        losses=losses)\n",
        "        #loss after this epoch\n",
        "        print('Losses', losses)\n",
        "\n",
        "\n",
        "# Save model \n",
        "output_dir = Path('./models/spacy')\n",
        "if not output_dir.exists():\n",
        "    output_dir.mkdir()\n",
        "nlp.meta['name'] = 'SpaCy'  # rename model\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created blank english model\n",
            "0\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 158\n",
            "Losses {'ner': 62837.293273210526}\n",
            "1\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n",
            "batch size 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQYAyPPaEP9j"
      },
      "source": [
        "## Internal Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcSC--8rRGmJ"
      },
      "source": [
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = sp.load(output_dir)\n",
        "\n",
        "ground_truth = []\n",
        "predictions = []\n",
        "for test_text in TEST_FULL:\n",
        "    this_gt = []\n",
        "    for gt_ent in test_text[1]['entities']:\n",
        "        dataset = test_text[0][gt_ent[0]:gt_ent[1]-1]\n",
        "        if dataset not in this_gt:\n",
        "            this_gt.append(dataset)\n",
        "    ground_truth.append(this_gt)\n",
        "    \n",
        "    this_pred = []\n",
        "    sentences = test_text[0].split('.')\n",
        "    for sentence in sentences:\n",
        "        doc = nlp2(sentence)\n",
        "        for ent in doc.ents:\n",
        "            print(ent.label_, ent.text)\n",
        "            if len(ent.text) > 0 and ent.text not in this_pred:\n",
        "                this_pred.append(ent.text)\n",
        "    predictions.append(this_pred)\n",
        "    \n",
        "print(len(ground_truth))\n",
        "print(len(predictions))"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6Ko9f1SEney"
      },
      "source": [
        "for num in range(0, len(ground_truth)):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dswzcoa4F8GV",
        "outputId": "b7d30df3-8de6-4fa2-991f-2263e5277a53"
      },
      "source": [
        "import json\n",
        "\n",
        "test = json.load(open('./data/test/8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60.json'))\n",
        "\n",
        "this_pub = \"\"\n",
        "for section in test:\n",
        "  this_pub += section['text']\n",
        "\n",
        "sentences = this_pub.split('.')\n",
        "for sentence in sentences:\n",
        "  doc = nlp(sentence)\n",
        "  # print(\"Entities in '%s'\" % test_text)\n",
        "  if len(doc.ents) > 0:\n",
        "    for ent in doc.ents:\n",
        "        print(ent.label_, ent.text)\n",
        "\n",
        "\n",
        "# count = 0\n",
        "# for index, row in train.iterrows():\n",
        "#   print(count)\n",
        "#   this_pub = \"\"\n",
        "#   this_pub_id = './data/train/' + row['Id'] + '.json'\n",
        "#   this_data_lab = row['dataset_label']\n",
        "#   this_pub_json = json.load(open(this_pub_id))\n",
        "#   for section in this_pub_json:\n",
        "#     this_pub += section['text']\n",
        "#   this_ent_dict = {\"entities\": []}\n",
        "#   for m in re.finditer(this_data_lab, this_pub):\n",
        "#     this_ent_dict['entities'].append((m.start(), m.end()+1, LABEL))\n",
        "#   SPACY_TRAIN.append((this_pub, this_ent_dict))\n",
        "#   count+=1\n",
        "\n",
        "# SPACY_TRAIN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data Rural-Urban Continuum Codes (\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0OgManJMvpQ"
      },
      "source": [
        "this_pub = \"\"\n",
        "for section in data:\n",
        "  this_pub += section['text']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMe9BLwON2_5",
        "outputId": "4ad49908-7b3f-48af-da4a-3c2b163c26db"
      },
      "source": [
        "import re\n",
        "this_data_lab = \"National Education Longitudinal Study\"\n",
        "for m in re.finditer(this_data_lab, this_pub):\n",
        "  this_pub[m.start():m.end()+1]\n",
        "print(this_pub[2937:2975])\n",
        "#[0, 5, 10, 15]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ely to attain a college degree than co\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb4M2hUlFs8o"
      },
      "source": [
        "# !pip install -U spacy\n",
        "# !python -m spacy download en\n",
        "\n",
        "#check num json files in train (should be 14,316)\n",
        "# import glob\n",
        "# len(glob.glob1(\"./data/train\", \"*.json\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}